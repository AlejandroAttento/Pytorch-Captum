{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# POC Explained AI using pytorch and captum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Custom\n",
    "from tool_box.deviceHandler import deviceHandler\n",
    "from tool_box.model import simpleDenseNN\n",
    "from tool_box.utilities import createDataLoader, listSplitter, type_converter, secondsConverter, createLog\n",
    "\n",
    "# Set timestamp for run ID\n",
    "run_timestamp = datetime.datetime.now()"
   ]
  },
  {
   "source": [
    "## Set up processing device"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_handler = deviceHandler()"
   ]
  },
  {
   "source": [
    "## Process data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dataset = pd.read_csv('titanic_dataset.csv')"
   ]
  },
  {
   "source": [
    "### Simple data processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies for categorical variables\n",
    "sex_dummies = pd.get_dummies(titanic_dataset.Sex, prefix='Sex')\n",
    "embarked_dummies = pd.get_dummies(titanic_dataset.Embarked, prefix='Embarked')\n",
    "pclass_dummies = pd.get_dummies(titanic_dataset.Pclass.astype(str), prefix='Pclass')\n",
    "\n",
    "# Fill NaNs\n",
    "titanic_dataset.Age = titanic_dataset.Age.fillna(round(titanic_dataset.Age.mean(), 1))\n",
    "titanic_dataset.Fare = titanic_dataset.Fare.fillna(round(titanic_dataset.Fare.mean(), 1))"
   ]
  },
  {
   "source": [
    "### Create features and target datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat([titanic_dataset[['Age', 'SibSp', 'Parch', 'Fare']], pclass_dummies, sex_dummies, embarked_dummies], axis=1)\n",
    "target_df = titanic_dataset.Survived"
   ]
  },
  {
   "source": [
    "### Scale data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_features_df = scaler.fit_transform(features_df)"
   ]
  },
  {
   "source": [
    "### Transform into tuple list of pytorch tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tensor = device_handler.data_to_tensor(s_features_df)\n",
    "target_tensor = device_handler.data_to_tensor(target_df).reshape(-1, 1)\n",
    "\n",
    "features_tensor = type_converter(features_tensor, torch.float32)\n",
    "target_tensor = type_converter(target_tensor, torch.float32)\n",
    "\n",
    "tuple_lst_data = list(zip(features_tensor, target_tensor))"
   ]
  },
  {
   "source": [
    "### Split data into test and training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_splitter = listSplitter(0.3, shuffle = True)\n",
    "test_data, train_data = lst_splitter.split(tuple_lst_data)"
   ]
  },
  {
   "source": [
    "### Create Dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_gen = createDataLoader()\n",
    "\n",
    "test_loader = dataloader_gen.create(test_data, batch_size=50)\n",
    "train_loader = dataloader_gen.create(train_data, batch_size=50, shuffle = True)"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Define model parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = simpleDenseNN(features_tensor.shape[1], features_tensor.shape[1]*2+1, 1)\n",
    "\n",
    "opt = torch.optim.Adam(simple_model.parameters(), 1e-02)"
   ]
  },
  {
   "source": [
    "### Run model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "batch_cum = 0 \n",
    "epoch_amt = 100\n",
    "start_time_VAL = time.time()\n",
    "run_results = []\n",
    "model_DICT = {}\n",
    "\n",
    "for epoch in range(epoch_amt):\n",
    "  \n",
    "  train_loss = []\n",
    "  train_acc = []\n",
    "  train_f1 = []\n",
    "  train_prec = []\n",
    "  train_rec = []\n",
    "  train_auc = []\n",
    "  \n",
    "  for _i, batch in enumerate(train_loader):\n",
    "    \n",
    "    preds = simple_model.training_step(batch)\n",
    "    \n",
    "    train_loss.append(preds['loss'])\n",
    "    train_acc.append(preds['acc'])\n",
    "    train_f1.append(preds['f1'])\n",
    "    train_prec.append(preds['prec'])\n",
    "    train_rec.append(preds['rec'])\n",
    "    \n",
    "    preds['loss'].backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "  validation_result = [simple_model.testing_step(batch) for batch in test_loader]\n",
    "  \n",
    "  epoch_train_loss = np.mean([val.item() for val in train_loss])\n",
    "  epoch_train_acc = np.mean([val.item() for val in train_acc])\n",
    "  epoch_train_f1 = np.mean([val.item() for val in train_f1])\n",
    "  epoch_train_prec = np.mean([val.item() for val in train_prec])\n",
    "  epoch_train_rec = np.mean([val.item() for val in train_rec])\n",
    "  \n",
    "  epoch_test_loss = np.mean([dic['loss'].item() for dic in validation_result])\n",
    "  epoch_test_acc = np.mean([dic['acc'].item() for dic in validation_result])\n",
    "  epoch_test_f1 = np.mean([dic['f1'].item() for dic in validation_result])\n",
    "  epoch_test_prec = np.mean([dic['prec'].item() for dic in validation_result])\n",
    "  epoch_test_rec = np.mean([dic['rec'].item() for dic in validation_result])\n",
    "  \n",
    "  epoch_results = {'run_id': run_timestamp.strftime('%Y%m%d%H%M%S'), 'calendar_dt': run_timestamp.strftime('%Y-%m-%d'),\\\n",
    "                   'training_cases': len(train_data), 'testing_cases': len(test_data), 'epoch': epoch+1, 'total_epochs': epoch_amt,\\\n",
    "                   'training_run' : {'loss': epoch_train_loss, 'accuracy': epoch_train_acc, 'f1': epoch_train_f1,\\\n",
    "                                     'precision': epoch_train_prec, 'recall': epoch_train_rec},\\\n",
    "                   'test_run' : {'loss': epoch_test_loss, 'accuracy': epoch_test_acc, 'f1': epoch_test_f1,\\\n",
    "                                 'precision': epoch_test_prec, 'recall': epoch_test_rec}}\n",
    "  \n",
    "  run_results.append(epoch_results)\n",
    "  \n",
    "  print('{} Message time: Epoch {}/{} processed - {} time passed'\\\n",
    "    .format(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), epoch+1, epoch_amt, secondsConverter(time.time()-start_time_VAL)))\n",
    "  \n",
    "  print('TRAINING\\tLoss: {:.5f} | Accuracy: {:.2f}% | F1: {:.2f} | Precision: {:.2f} | Recall: {:.2f}'\\\n",
    "    .format(epoch_train_loss.item(), epoch_train_acc * 100, epoch_train_f1, epoch_train_prec, epoch_train_rec))\n",
    "                 \n",
    "  print('TESTING\\t\\tLoss: {:.5f} | Accuracy: {:.2f}% | F1: {:.2f} | Precision: {:.2f} | Recall: {:.2f}'\\\n",
    "    .format(epoch_test_loss.item(), epoch_test_acc * 100, epoch_test_f1, epoch_test_prec, epoch_test_rec) + '\\n')"
   ]
  },
  {
   "source": [
    "### Save log file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_log = createLog(run_results, json = True, file_name = run_timestamp.strftime('%Y%m%d%H%M%S'))\n",
    "create_log.write()"
   ]
  },
  {
   "source": [
    "## Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Process results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(run_results)\n",
    "\n",
    "training_org_cols = [col for col in list(results_df['training_run'][0].keys())]\n",
    "test_org_cols = [col for col in list(results_df['test_run'][0].keys())]\n",
    "\n",
    "training_cols = ['train_' + col for col in list(results_df['training_run'][0].keys())]\n",
    "test_cols = ['test_' + col for col in list(results_df['test_run'][0].keys())]\n",
    "\n",
    "\n",
    "train_cols_dict = dict()\n",
    "test_cols_dict = dict()\n",
    "\n",
    "for _i in range(len(training_cols)):\n",
    "  train_cols_dict[training_org_cols[_i]] = training_cols[_i]\n",
    "  \n",
    "for _i in range(len(test_cols)):\n",
    "  test_cols_dict[test_org_cols[_i]] = test_cols[_i]\n",
    "  \n",
    "\n",
    "training_df = pd.DataFrame(list(results_df['training_run'])).rename(columns=train_cols_dict)\n",
    "testing_df = pd.DataFrame(list(results_df['test_run'])).rename(columns=test_cols_dict)\n",
    "\n",
    "formatted_results_df = pd.concat([results_df.drop(['training_run', 'test_run'], axis=1), training_df, testing_df], axis=1)"
   ]
  },
  {
   "source": [
    "### Graph loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['train_loss'], name= 'Training', mode='lines+markers', line_color='blue'))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['test_loss'], name= 'Testing', mode='lines+markers', line_color='red'))\n",
    "fig.update_layout(\n",
    "    title=\"Loss progression\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Loss\",\n",
    "    font=dict(size=15),\n",
    "    width=1300, \n",
    "    height=600)\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "### Graph evaluation metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['train_accuracy'], name= 'Training Accuracy',\\\n",
    "                         opacity=0.3, mode='lines+markers', line_color='blue', line_width= 2))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['train_f1'], name= 'Training F1',\\\n",
    "                         opacity=0.3, mode='lines+markers', line_color='red', line_width= 2))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['train_precision'], name= 'Training Precision',\\\n",
    "                         opacity=0.3, mode='lines+markers', line_color='gold', line_width= 2))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['train_recall'], name= 'Training Recall',\\\n",
    "                         opacity=0.3, mode='lines+markers', line_color='green', line_width= 2))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['test_accuracy'], name= 'Testing Accuracy',\\\n",
    "                         mode='lines+markers', line_color='blue', line_width= 3))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['test_f1'], name= 'Testing F1',\\\n",
    "                         mode='lines+markers', line_color='red', line_width= 3))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['test_precision'], name= 'Testing Precision',\\\n",
    "                         mode='lines+markers', line_color='gold', line_width= 3))\n",
    "fig.add_trace(go.Scatter(x=formatted_results_df['epoch'], y=formatted_results_df['test_recall'], name= 'Testing Recall',\\\n",
    "                         mode='lines+markers', line_color='green', line_width= 3))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Evaluation metrics\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    font=dict(size=15),\n",
    "    width=1300, \n",
    "    height=600)\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "## Explainable IA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Single prediction interpretation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Calculate single case attribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(simple_model)\n",
    "\n",
    "attributions, approximation_error = ig.attribute(batch[0][0:1], target = 0, return_convergence_delta = True)"
   ]
  },
  {
   "source": [
    "### Attribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = features_df.columns.tolist()\n",
    "\n",
    "attributions_lst = list(zip(features_names, attributions.numpy().tolist()[0]))\n",
    "\n",
    "for feature in attributions_lst:\n",
    "    print('{:<10}:\\t{:>6.3f}'.format(feature[0], feature[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(x = features_names, y = attributions.numpy().tolist()[0])])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Attribution\",\n",
    "    xaxis_title=\"Features\",\n",
    "    font=dict(size=15),\n",
    "    width=1300, \n",
    "    height=600)\n",
    "\n",
    "fig.show()"
   ]
  }
 ]
}
