{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# POC Explained IA using pytorch and captum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our ML things\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import IntegratedGradients # Most popular atribution methode\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, auc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom\n",
    "from tool_box.deviceHandler import deviceHandler\n",
    "from tool_box.model import simpleDenseNN\n",
    "from tool_box.utilities import createDataLoader, listSplitter, type_converter, secondsConverter\n",
    "\n",
    "run_timestamp = datetime.datetime.now()"
   ]
  },
  {
   "source": [
    "## Set up processing device"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU isn't available, fallback to CPU\n"
     ]
    }
   ],
   "source": [
    "device_handler = deviceHandler()"
   ]
  },
  {
   "source": [
    "## Process data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dataset = pd.read_csv('titanic_dataset.csv')"
   ]
  },
  {
   "source": [
    "### Simple data processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies for categorical variables\n",
    "sex_dummies = pd.get_dummies(titanic_dataset.Sex, prefix='Sex')\n",
    "embarked_dummies = pd.get_dummies(titanic_dataset.Embarked, prefix='Embarked')\n",
    "pclass_dummies = pd.get_dummies(titanic_dataset.Pclass.astype(str), prefix='Pclass')\n",
    "\n",
    "# Fill NaNs\n",
    "titanic_dataset.Age = titanic_dataset.Age.fillna(round(titanic_dataset.Age.mean(), 1))\n",
    "titanic_dataset.Fare = titanic_dataset.Fare.fillna(round(titanic_dataset.Fare.mean(), 1))"
   ]
  },
  {
   "source": [
    "### Create features and target datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat([titanic_dataset[['Age', 'SibSp', 'Parch', 'Fare']], pclass_dummies, sex_dummies, embarked_dummies], axis=1)\n",
    "target_df = titanic_dataset.Survived"
   ]
  },
  {
   "source": [
    "### Scale data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s_features_df = scaler.fit_transform(features_df)"
   ]
  },
  {
   "source": [
    "### Transform into tuple list of pytorch tensors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tensor = device_handler.data_to_tensor(s_features_df)\n",
    "target_tensor = device_handler.data_to_tensor(target_df).reshape(-1, 1)\n",
    "\n",
    "features_tensor = type_converter(features_tensor, torch.float32)\n",
    "target_tensor = type_converter(target_tensor, torch.float32)\n",
    "\n",
    "tuple_lst_data = list(zip(features_tensor, target_tensor))"
   ]
  },
  {
   "source": [
    "### Split data into test and training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_splitter = listSplitter(0.3, shuffle = True)\n",
    "test_data, train_data = lst_splitter.split(tuple_lst_data)"
   ]
  },
  {
   "source": [
    "### Create Dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_gen = createDataLoader()\n",
    "\n",
    "test_loader = dataloader_gen.create(test_data, batch_size=150)\n",
    "train_loader = dataloader_gen.create(train_data, batch_size=150, shuffle = True)"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Define model parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = simpleDenseNN(features_tensor.shape[1], features_tensor.shape[1]*2+1, 1)\n",
    "\n",
    "opt = torch.optim.Adam(simple_model.parameters(), 1e-02)"
   ]
  },
  {
   "source": [
    "### Run model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'secondsConverter' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e0ccd5d1a727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[0mrun_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} Message time: Epoch {}/{} processed - {} time passed'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d/%m/%Y %H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_amt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondsConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time_VAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TRAINING   Loss: {:.5f} | Accuracy: {:.2f}% | F1: {:.2f} | Precision: {:.2f} | Recall: {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_train_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_train_acc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_train_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_train_prec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_train_rec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'secondsConverter' is not defined"
     ]
    }
   ],
   "source": [
    "batch_cum = 0 \n",
    "epoch_amt = 5\n",
    "start_time_VAL = time.time()\n",
    "run_results = []\n",
    "model_DICT = {}\n",
    "\n",
    "for epoch in range(epoch_amt):\n",
    "  \n",
    "  train_loss = []\n",
    "  train_acc = []\n",
    "  train_f1 = []\n",
    "  train_prec = []\n",
    "  train_rec = []\n",
    "  train_auc = []\n",
    "  \n",
    "  for _i, batch in enumerate(train_loader):\n",
    "    \n",
    "    preds = simple_model.training_step(batch)\n",
    "    \n",
    "    train_loss.append(preds['loss'])\n",
    "    train_acc.append(preds['acc'])\n",
    "    train_f1.append(preds['f1'])\n",
    "    train_prec.append(preds['prec'])\n",
    "    train_rec.append(preds['rec'])\n",
    "    \n",
    "    preds['loss'].backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "  validation_result = [simple_model.testing_step(batch) for batch in test_loader]\n",
    "  \n",
    "  epoch_train_loss = np.mean([val.item() for val in train_loss])\n",
    "  epoch_train_acc = np.mean([val.item() for val in train_acc])\n",
    "  epoch_train_f1 = np.mean([val.item() for val in train_f1])\n",
    "  epoch_train_prec = np.mean([val.item() for val in train_prec])\n",
    "  epoch_train_rec = np.mean([val.item() for val in train_rec])\n",
    "  \n",
    "  epoch_test_loss = np.mean([dic['loss'].item() for dic in validation_result])\n",
    "  epoch_test_acc = np.mean([dic['acc'].item() for dic in validation_result])\n",
    "  epoch_test_f1 = np.mean([dic['f1'].item() for dic in validation_result])\n",
    "  epoch_test_prec = np.mean([dic['prec'].item() for dic in validation_result])\n",
    "  epoch_test_rec = np.mean([dic['rec'].item() for dic in validation_result])\n",
    "  \n",
    "  epoch_results = {'run_id': run_timestamp.strftime('%Y%m%d%H%M%S'), 'calendar_dt': run_timestamp.strftime('%Y-%m-%d'),\\\n",
    "                   'training_cases': len(train_data), 'testing_cases': len(test_data), 'epoch': epoch+1, 'total_epochs': epoch_amt,\\\n",
    "                   'training_run' : {'loss': epoch_train_loss, 'accuracy': epoch_train_acc, 'f1': epoch_train_f1,\\\n",
    "                                     'precision': epoch_train_prec, 'recall': epoch_train_rec},\\\n",
    "                   'validation_run' : {'loss': epoch_test_loss, 'accuracy': epoch_test_acc, 'f1': epoch_test_f1,\\\n",
    "                                       'precision': epoch_test_prec, 'recall': epoch_test_rec}}\n",
    "  \n",
    "  run_results.append(epoch_results)\n",
    "  \n",
    "  print('{} Message time: Epoch {}/{} processed - {} time passed'.format(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), epoch+1, epoch_amt, secondsConverter(time.time()-start_time_VAL)))\n",
    "  print('TRAINING   Loss: {:.5f} | Accuracy: {:.2f}% | F1: {:.2f} | Precision: {:.2f} | Recall: {:.2f}'.format(epoch_train_loss.item(), epoch_train_acc * 100, epoch_train_f1, epoch_train_prec, epoch_train_rec))\n",
    "                 \n",
    "  print('VALIDATION Loss: {:.5f} | Accuracy: {:.2f}% | F1: {:.2f} | Precision: {:.2f} | Recall: {:.2f}'.format(epoch_validation_loss.item(), epoch_validation_acc * 100, epoch_validation_f1, epoch_validation_prec, epoch_validation_rec) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'loss': tensor(0.4541, grad_fn=<BinaryCrossEntropyBackward>),\n",
       "  'acc': 0.8266666666666667,\n",
       "  'f1': 0.7936507936507936,\n",
       "  'prec': 0.847457627118644,\n",
       "  'rec': 0.746268656716418},\n",
       " {'loss': tensor(0.3389, grad_fn=<BinaryCrossEntropyBackward>),\n",
       "  'acc': 0.8866666666666667,\n",
       "  'f1': 0.8210526315789473,\n",
       "  'prec': 0.8478260869565217,\n",
       "  'rec': 0.7959183673469388},\n",
       " {'loss': tensor(0.3002, grad_fn=<BinaryCrossEntropyBackward>),\n",
       "  'acc': 0.8913043478260869,\n",
       "  'f1': 0.8214285714285714,\n",
       "  'prec': 0.92,\n",
       "  'rec': 0.7419354838709677}]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "validation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}